{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES IMPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ragas\n",
    "import openai\n",
    "import langchain\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULES IMPORT\n",
    "from query_chatbot_by_retriever import load_database\n",
    "from get_chunk_stats import reconstruct_chunks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 17:45:05.218 | INFO     | query_chatbot_by_retriever:load_database:179 - Loading the vector database.\n",
      "2024-05-21 17:45:05.267 | INFO     | query_chatbot_by_retriever:load_database:185 - Chunks in the database: 1312\n",
      "2024-05-21 17:45:05.268 | SUCCESS  | query_chatbot_by_retriever:load_database:187 - Vector database prepared successfully.\n",
      "\n",
      "2024-05-21 17:45:05.269 | INFO     | get_chunk_stats:reconstruct_chunks:224 - Reconstructing the chunks from the vector database...\n",
      "2024-05-21 17:45:05.347 | SUCCESS  | get_chunk_stats:reconstruct_chunks:238 - Reconstructed 1312 chunks from the vector database successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Chroma database\n",
    "vector_db_path = \"../chroma_db\"\n",
    "vector_db = load_database(vector_db_path)[0]\n",
    "\n",
    "# Get the chunks\n",
    "chunks = reconstruct_chunks(vector_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_135824/966000715.py:1: DeprecationWarning: The function with_openai was deprecated in 0.1.4, and will be removed in the 0.2.0 release. Use from_langchain instead.\n",
      "  generator = TestsetGenerator.with_openai()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55a509657f042d7ad1c0c1ba8b3c7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/2624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/testset/extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/llms/base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/llms/base.py\", line 169, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 570, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 530, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 715, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 559, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1159, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tess01hp/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-jz94znQfKeWaSShoPVEIzHTA on tokens per min (TPM): Limit 60000, Used 59551, Requested 670. Please try again in 221ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m generator \u001b[39m=\u001b[39m TestsetGenerator\u001b[39m.\u001b[39mwith_openai()\n\u001b[0;32m----> 2\u001b[0m testset \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mgenerate_with_langchain_docs(chunks, test_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, distributions\u001b[39m=\u001b[39m{simple: \u001b[39m0.5\u001b[39m, reasoning: \u001b[39m0.25\u001b[39m, multi_context: \u001b[39m0.25\u001b[39m})\n\u001b[1;32m      3\u001b[0m testset\u001b[39m.\u001b[39mto_pandas()\n",
      "File \u001b[0;32m~/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/testset/generator.py:168\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[1;32m    166\u001b[0m distributions \u001b[39m=\u001b[39m distributions \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    167\u001b[0m \u001b[39m# chunk documents and add to docstore\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocstore\u001b[39m.\u001b[39madd_documents(\n\u001b[1;32m    169\u001b[0m     [Document\u001b[39m.\u001b[39mfrom_langchain_document(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    173\u001b[0m     test_size\u001b[39m=\u001b[39mtest_size,\n\u001b[1;32m    174\u001b[0m     distributions\u001b[39m=\u001b[39mdistributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     run_config\u001b[39m=\u001b[39mrun_config,\n\u001b[1;32m    179\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/testset/docstore.py:215\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[0;34m(self, docs, show_progress)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[1;32m    211\u001b[0m nodes \u001b[39m=\u001b[39m [\n\u001b[1;32m    212\u001b[0m     Node\u001b[39m.\u001b[39mfrom_langchain_document(d)\n\u001b[1;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplitter\u001b[39m.\u001b[39mtransform_documents(docs)\n\u001b[1;32m    214\u001b[0m ]\n\u001b[0;32m--> 215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_nodes(nodes, show_progress\u001b[39m=\u001b[39mshow_progress)\n",
      "File \u001b[0;32m~/anaconda3/envs/biopyassistantenv/lib/python3.11/site-packages/ragas/testset/docstore.py:254\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    252\u001b[0m results \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mresults()\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m results:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mraise\u001b[39;00m ExceptionInRunner()\n\u001b[1;32m    256\u001b[0m \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(nodes):\n\u001b[1;32m    257\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m nodes_to_embed\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "generator = TestsetGenerator.with_openai()\n",
    "testset = generator.generate_with_langchain_docs(chunks, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "testset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "for query in questions:\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(rag_chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopyassistantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85e9f3322d4fc95f5009a81da63171dd3d9c13ac3d28b79e3b21ee5afcc1e3f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragas Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas is a framework that helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. It provides several modules which come handy for evaluating RAG. The two main ones - and the ones we are using in this guide are:\n",
    "\n",
    "- *TestsetGenerator*: This module is responsible for generating test sets for evaluating RAG pipelines. It loads a bunch of documents or text chunks and then uses an LLM to generate potential questions based on these documents as well as answers for these questions - based on the provided documents. The generated answers are then used as \"ground truth\" to evaluate the RAG pipeline in a subsequent step.\n",
    "\n",
    "- *evaluate*: This module is responsible for evaluating RAG pipelines using the generated test sets. uses the questions of the generated test set to evaluate the RAG pipeline. It again uses an LLM to validate the answers given from your RAG pipeline based on the questions provided in the test set. The LLM also is asked to validate how good the provided contexts fit the questions. And finally, the LLM answer is compared to the ground truth answer (which is part of the test set) to evaluate the LLM itself. It provides a variety of evaluation metrics, including answer relevancy, faithfulness, context recall, and context precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES IMPORT\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from ragas import adapt\n",
    "from ragas import evaluate\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULES IMPORT\n",
    "from query_chatbot import load_database\n",
    "from query_chatbot import generate_answer, search_similarity_in_database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the database of vectors\n",
    "def load_database_from_path(vector_db_path: str) -> Chroma:\n",
    "    \"\"\" Load the database of vectors from a given path \"\"\"\n",
    "    # Load the database\n",
    "    vector_db = load_database(vector_db_path)[0]\n",
    "\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reconstruct the chunks from the vector database\n",
    "def reconstruct_chunks(vector_db: Chroma) -> list:\n",
    "    \"\"\"Reconstruct the chunks from the vector database.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vector_db : Chroma\n",
    "        The vector database to reconstruct the chunks.\n",
    "    Returns\n",
    "    -------\n",
    "    chunks : list of Document\n",
    "        List of text chunks reconstructed from the vector database.\n",
    "        format : [{\"page_content\": str, \"metadata\": dict}, ...]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    vector_collections = vector_db.get()\n",
    "    total_chunks = len(vector_collections[\"ids\"])\n",
    "    for i in range(total_chunks):\n",
    "        chunk = {\n",
    "            \"page_content\": vector_collections[\"documents\"][i],\n",
    "            \"metadata\": vector_collections[\"metadatas\"][i],\n",
    "        }\n",
    "        # ** used to unpack a dictionary into keyword arguments.\n",
    "        chunk = Document(**chunk)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    print(\n",
    "        f\"Reconstructed {total_chunks} chunks from the vector database successfully.\\n\"\n",
    "    )\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the testset\n",
    "def create_testset(chunks: list, generator_llm_name: str, critic_llm_name: str, language: str, nb_query: int = 10) -> DataFrame:\n",
    "    \"\"\"Create the testset for the evaluation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    chunks : list of Document\n",
    "        List of text chunks to generate the testset.\n",
    "    generator_llm_name : str\n",
    "        The name of the generator language model for simple questions.\n",
    "    critic_llm_name : str\n",
    "        The name of the critic language model for reasoning questions.\n",
    "    language : str\n",
    "        The language of the testset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    testset : DataFrame\n",
    "        The generated testset.\n",
    "    \"\"\"\n",
    "    # Get nb_query random chunks\n",
    "    selected_chunks = random.sample(chunks, nb_query)\n",
    "    \n",
    "    # Create the generator from openai models\n",
    "    generator_llm = ChatOpenAI(model=generator_llm_name)\n",
    "    critic_llm = ChatOpenAI(model= critic_llm_name)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    # Define the testset generator\n",
    "    generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, embeddings)\n",
    "    # adapt the generator to the French language\n",
    "    generator.adapt(language, evolutions=[simple, reasoning,conditional,multi_context])\n",
    "    # save the generator\n",
    "    generator.save(evolutions=[simple, reasoning, multi_context,conditional])\n",
    "\n",
    "    # Generate the testset\n",
    "    testset = generator.generate_with_langchain_docs(\n",
    "        selected_chunks, # chunks to generate the testset\n",
    "        test_size=nb_query, # number of samples to generate\n",
    "        distributions={simple: 0.4, reasoning: 0.2, multi_context: 0.2, conditional: 0.2} # distribution of the testset for each type of question\n",
    "    )\n",
    "    # convert the testset to a dataframe\n",
    "    testset_df = testset_df = testset.to_pandas()\n",
    "\n",
    "    return testset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the answers from the rag pipeline\n",
    "def get_answers(testset_df: DataFrame, vector_db: Chroma, llm_name: str) -> DataFrame:\n",
    "    \"\"\"Get the answers from the RAG pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    testset_df : DataFrame\n",
    "        The testset dataframe.\n",
    "    vector_db : Chroma\n",
    "        The vector database.\n",
    "    llm_name : str\n",
    "        The name of the language model to use for the answer generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataset : DataFrame\n",
    "        The dataset containing the questions, answers, contexts, and ground truth.\n",
    "    \"\"\"\n",
    "    # Create list of questions and ground truth\n",
    "    questions = testset_df[\"question\"].to_list()\n",
    "    ground_truth = testset_df[\"ground_truth\"].to_list()\n",
    "\n",
    "    # Initialize the data dictionary\n",
    "    data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "    # Generate answers for the questions and store query, answer, and relevant contexts in the data dictionary\n",
    "    for query in questions:\n",
    "        relevant_chunks = search_similarity_in_database(vector_db, query, 3, logger_flag=False)\n",
    "        answer = generate_answer(query, \"\", relevant_chunks, llm_name, logger_flag=False)\n",
    "        data[\"question\"].append(query)\n",
    "        data[\"answer\"].append(answer)\n",
    "        data[\"contexts\"].append([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "    # Create a dataset from the data dictionary\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    # Convert the dataset to pandas dataframe\n",
    "    dataset.to_pandas()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the answers\n",
    "def evaluate_answers(dataset: DataFrame, metrics) -> dict:\n",
    "    \"\"\"Evaluate the answers using the RAG evaluation metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : DataFrame\n",
    "        The dataset containing the questions, answers, contexts, and ground truth.\n",
    "    metrics : list\n",
    "        The list of evaluation metrics to use.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        The evaluation results.\n",
    "    \"\"\"\n",
    "    # Evaluate the dataset\n",
    "    evaluation = evaluate(\n",
    "        dataset = dataset,\n",
    "        metrics = metrics\n",
    "    )\n",
    "    print(f\"For the testset, the evaluation results are as follows:\\n\")\n",
    "    print(evaluation)\n",
    "\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the evaluation results\n",
    "def plot_evaluation_results(evaluation_results: DataFrame, metrics: list):\n",
    "    \"\"\"Plot the evaluation results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluation_results : DataFrame\n",
    "        The evaluation results dataframe.\n",
    "    metrics : list\n",
    "        The list of evaluation metrics to plot.\n",
    "    \"\"\"\n",
    "    # Create a heatmap of the evaluation results\n",
    "    heatmap_data = evaluation_results[metrics]\n",
    "    # Gradient color map\n",
    "    cmap = LinearSegmentedColormap.from_list('green_red', ['red', 'green'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", linewidths=.5, cmap=cmap)\n",
    "\n",
    "    # Create labels for y-ticks as \"chunks {index+1}\"\n",
    "    y_labels = [f'chunks {i+1}' for i in evaluation_results.index]\n",
    "    # Use these labels for y-ticks\n",
    "    plt.yticks(ticks=range(len(evaluation_results.index)), labels=y_labels, rotation=0)\n",
    "    # save the plot\n",
    "    plt.savefig(\"evaluation_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze the impact of the language model on the evaluation\n",
    "def analysis_llm_impact(testset: DataFrame, vector_db: Chroma, llm_models: list, metrics: list) -> dict:\n",
    "    \"\"\"Analyze the impact of the language model on the evaluation of the RAG pipeline.\"\"\"\n",
    "    evaluation_df = {}\n",
    "    for llm_name in llm_models:\n",
    "        answers_dataset = get_answers(testset, vector_db, llm_name)\n",
    "        print(f\"Answers generated using the {llm_name} language model.\\n\")\n",
    "        evaluation_results = evaluate_answers(answers_dataset, metrics)\n",
    "        evaluation_df[llm_name] = evaluation_results\n",
    "    \n",
    "    return evaluation_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the analysis of the impact of the language model on the evaluation\n",
    "def plot_analysis_llm_impact(evaluation_dfs: dict, metrics: list):\n",
    "    \"\"\"\n",
    "    Analyze and plot the distribution of evaluation metrics for multiple LLM models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluation_dfs : dict\n",
    "        A dictionary where keys are model names and values are DataFrames containing evaluation metrics.\n",
    "    metrics : list\n",
    "        A list of evaluation metrics to plot.\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    model_names = list(evaluation_dfs.keys())\n",
    "    model_eval = list(evaluation_dfs.values())\n",
    "    model_df = [model_eval.to_pandas() for model_eval in model_eval]\n",
    "    model_eval_df = [model_df[metrics] for model_df in model_df]\n",
    "\n",
    "    _fig, axs = plt.subplots(1, len(metrics), figsize=(20, 5))\n",
    "   \n",
    "    for i, col in enumerate(model_eval_df[0].columns):\n",
    "        sns.kdeplot(data=[model_eval_df[col].values for model_eval_df in model_eval_df],legend=False,ax=axs[i],fill=True)\n",
    "        axs[i].set_title(f'{col} scores distribution')\n",
    "        axs[i].legend(labels=model_names)\n",
    "    plt.tight_layout()\n",
    "    # save the plot\n",
    "    plt.savefig(\"evaluation_distribution_llms.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and create testsets\n",
    "def load_and_create_testsets(db_paths, num_samples=50, generator_llm_name=\"gpt-3.5-turbo\", critic_llm_name=\"gpt-4o\", language=\"fr\"):\n",
    "    \"\"\"Load databases, reconstruct chunks, randomly sample chunks, and create testsets.\"\"\"\n",
    "    vector_dbs = []\n",
    "    testsets_size = []\n",
    "    \n",
    "    for db_path in db_paths:\n",
    "        # Load the database\n",
    "        vector_db = load_database_from_path(db_path)\n",
    "        vector_dbs.append(vector_db)\n",
    "        \n",
    "        # Reconstruct the chunks\n",
    "        chunks = reconstruct_chunks(vector_db)\n",
    "        \n",
    "        # Randomly sample chunks\n",
    "        random_chunks = np.random.choice(chunks, num_samples, replace=False)\n",
    "        \n",
    "        # Create the testset\n",
    "        testset_df = create_testset(\n",
    "            chunks=random_chunks,\n",
    "            generator_llm_name=generator_llm_name,\n",
    "            critic_llm_name=critic_llm_name,\n",
    "            language=language\n",
    "        )\n",
    "        \n",
    "        # Store the testset in the dictionary\n",
    "        testsets_size.append(testset_df)\n",
    "    \n",
    "    return vector_dbs, testsets_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze the impact of the vector database on the evaluation\n",
    "def analysis_size_impact(testsets: list, vectors_db: list, llm_model: str, metrics: list) -> dict:\n",
    "    \"\"\"Analyze the impact of the vector database on the evaluation of the RAG pipeline.\"\"\"\n",
    "    size = [200, 500, 1000, 2000, 3000]\n",
    "    evaluation_size_df = {}\n",
    "    for i in range(len(testsets)):\n",
    "        answers_dataset = get_answers(testsets[i], vectors_db[i], llm_model, logger_flag=False)\n",
    "        print(f\"Answers generated using the {llm_model} language model.\\n\")\n",
    "        evaluation_df = evaluate_answers(answers_dataset, metrics)\n",
    "        evaluation_size_df[f\"db_{size[i]}\"] = evaluation_df\n",
    "    \n",
    "    return evaluation_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the analysis of the impact of the vector database on the evaluation\n",
    "def plot_analysis_size_impact(evaluation_dfs: dict, metrics: list):\n",
    "    \"\"\"\n",
    "    Analyze and plot the distribution of evaluation metrics for multiple vector databases.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluation_dfs : dict\n",
    "        A dictionary where keys are vector database names and values are DataFrames containing evaluation metrics.\n",
    "    metrics : list\n",
    "        A list of evaluation metrics to plot.\n",
    "    \"\"\"\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    db_names = list(evaluation_dfs.keys())\n",
    "    db_df = list(evaluation_dfs.values())\n",
    "    db_eval_df = [db_df[metrics] for db_df in db_df]\n",
    "\n",
    "    _fig, axs = plt.subplots(1, len(metrics), figsize=(20, 5))\n",
    "   \n",
    "    for i, col in enumerate(db_eval_df[0].columns):\n",
    "        sns.kdeplot(data=[db_eval_df[col].values for db_eval_df in db_eval_df],legend=False,ax=axs[i],fill=True)\n",
    "        axs[i].set_title(f'{col} scores distribution')\n",
    "        axs[i].legend(labels=db_names)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:34:24.923 | INFO     | query_chatbot:load_database:166 - Loading the vector database.\n",
      "2024-06-04 20:34:25.692 | INFO     | query_chatbot:load_database:176 - Chunks in the database: 890\n",
      "2024-06-04 20:34:25.693 | SUCCESS  | query_chatbot:load_database:178 - Vector database prepared successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Chroma database\n",
    "vector_db = load_database_from_path(\"../chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed 890 chunks from the vector database successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the chunks\n",
    "chunks = reconstruct_chunks(vector_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e616d4b87f429ead5cd615d14b6ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset_df = create_testset(chunks=chunks, generator_llm_name=\"gpt-4o\", critic_llm_name=\"gpt-4o\", language=\"fr\", nb_query=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the testset\n",
    "testset_df.to_csv(\"testset.csv\", index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "testset_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the answers from the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dataset = get_answers(testset_df, vector_db, llm_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df = answers_dataset.to_pandas()\n",
    "# Save the DataFrame to a CSV file with special characters handled\n",
    "answers_df.to_csv('answers_df.csv', index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "# Display it\n",
    "answers_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [faithfulness, context_precision, context_recall, answer_relevancy]\n",
    "evaluation_results = evaluate_answers(answers_dataset, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Faithfulness** measures the factual accuracy of the generated answer. The number of correct statements from the given contexts is divided by the total number of statements in the generated answer. This metric uses the question, contextsand the answer.\n",
    "\n",
    "- **Context precision** measures the signal-to-noise ratio of the retrieved context. This metric is computed using the question and the contexts.\n",
    "\n",
    "- **Context recall** measures if all the relevant information required to answer the question was retrieved. This metric is computed based on the ground_truth (this is the only metric in the framework that relies on human-annotated ground truth labels) and the contexts.\n",
    "\n",
    "- **Answer relevancy** measures how relevant the generated answer is to the question. This metric is computed using the question and the answer. For example, the answer “France is in western Europe.” to the question “Where is France and what is it’s capital?” would achieve a low answer relevancy because it only answers half of the question.\n",
    "\n",
    "All metrics are scaled to the range [0, 1], with higher values indicating a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = evaluation_results.to_pandas()\n",
    "# Save the DataFrame to a CSV file with special characters handled\n",
    "evaluation_df.to_csv('evaluation_df.csv', index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "evaluation_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to plot\n",
    "metrics_str = ['faithfulness', 'context_precision', 'context_recall', 'answer_relevancy']\n",
    "plot_evaluation_results(evaluation_df, metrics_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare the performance of the RAG pipeline with differentes openai models. We will use the same test set for all the models and then compare the results.\n",
    "\n",
    "This will answer the questions:\n",
    "> Is the performance of the RAG pipeline model dependent on the underlying LLM model? And if so, which model performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = [\"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-4-turbo\"]\n",
    "evaluations_llm_df = analysis_llm_impact(testset_df, vector_db, llm_models, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluation dataframes for different language models:\\n\")\n",
    "print(f\"Language model: gpt-3.5-turbo\")\n",
    "eval_df_gpt_3_5_turbo = list(evaluations_llm_df.values())[0].to_pandas()\n",
    "# Save the DataFrame to a CSV file with special characters handled\n",
    "eval_df_gpt_3_5_turbo.to_csv('eval_df_gpt_3_5_turbo.csv', index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "eval_df_gpt_3_5_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Language model: gpt-4o\")\n",
    "eval_df_gpt_4o = list(evaluations_llm_df.values())[1].to_pandas()\n",
    "# Save the DataFrame to a CSV file with special characters handled\n",
    "eval_df_gpt_4o.to_csv('eval_df_gpt_4o.csv', index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "eval_df_gpt_4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Language model: gpt-4-turbo\")\n",
    "eval_df_gpt_4_turbo = list(evaluations_llm_df.values())[2].to_pandas()\n",
    "# Save the DataFrame to a CSV file with special characters handled\n",
    "eval_df_gpt_4_turbo.to_csv('eval_df_gpt_4_turbo.csv', index=False, encoding='utf-8', sep=',', quoting=1)\n",
    "eval_df_gpt_4_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the analysis of the impact of the language model on the evaluation\n",
    "plot_analysis_llm_impact(evaluations_llm_df, metrics_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will compare the performance of the RAG pipeline using different test sets, each generated with a different database. Each database will be created with a different chunk size.\n",
    "\n",
    "This will answer the questions:\n",
    "> Is the choice of the chunk size for the test set generation impacting the performance of the RAG pipeline? And if so, which chunk size performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# List of database paths\n",
    "db_paths = [\n",
    "    \"../chroma_db_500\",\n",
    "    \"../chroma_db_1000\",\n",
    "    \"../chroma_db_2000\",\n",
    "    \"../chroma_db_3000\"\n",
    "]\n",
    "\n",
    "# Create testsets for all databases\n",
    "vector_dbs, testsets_sizes = load_and_create_testsets(db_paths)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Generate answers and evaluate them for all databases\n",
    "evaluations_size_df = analysis_size_impact(testset_df, llm_model=\"gpt-4o\", metrics=metrics)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Plot the analysis of the impact of the vector database on the evaluation\n",
    "plot_analysis_size_impact(evaluations_size_df, metrics_str)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopyassistantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85e9f3322d4fc95f5009a81da63171dd3d9c13ac3d28b79e3b21ee5afcc1e3f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
